{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEOtINgnsNVw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lunch Time Python\n",
    "\n",
    "## 25.11.2022: spaCy\n",
    "<img style=\"width: 600px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/SpaCy_logo.svg\">\n",
    "\n",
    "[spaCy](https://spacy.io/) is an open-source natural language processing library written in Python and Cython.\n",
    "\n",
    "spaCy focuses on production usage and is very fast and efficient. It also supports deep learning workflows through interfacing with [TensorFlow](https://www.tensorflow.org/) or [PyTorch](https://pytorch.org/), as well as the transformer model library [Hugging Face](https://github.com/huggingface).\n",
    "\n",
    "*Press `Spacebar` to go to the next slide (or `?` to see all navigation shortcuts)*\n",
    "\n",
    "[Lunch Time Python](https://ssciwr.github.io/lunch-time-python/), [Scientific Software Center](https://ssc.iwr.uni-heidelberg.de), [Heidelberg University](https://www.uni-heidelberg.de/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 0 What to do with spaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "spaCy is very powerful for text annotation:\n",
    "- sentencize and tokenize\n",
    "- POS (part-of-speech) and lemma\n",
    "- NER (named entity recognition)\n",
    "- dependency parsing\n",
    "- text classification\n",
    "- morphological analysis\n",
    "spaCy can also learn new tasks through integraton with your machine learning stack. It also provides multi-task learning with pretrained transformers like [BERT](https://arxiv.org/abs/1810.04805). \n",
    "(BERT is used in the google search engine.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\n",
    "    \"The Scientific Software Center offers lunch-time Python - an informal way to learn about new Python libraries.\"\n",
    ")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4094EXrlsmY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1 Install spaCy\n",
    "You can install spaCy using `pip`:\n",
    "\n",
    "`pip install spacy`\n",
    "\n",
    "It is also available via `conda-forge`:\n",
    "\n",
    "`conda install -c conda-forge spacy`\n",
    "\n",
    "After installing spaCy, you also need to download the language model. For a medium-sized English model, you would do this using\n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "The available models are listed on the spaCy website: https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Install spaCy with CUDA support\n",
    "\n",
    "`pip install -U spacy[cuda]`\n",
    "\n",
    "You can also explore the [online tool](https://spacy.io/usage) for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "earp3FM4lg53",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2 Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp(\"This is lunch-time Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"This is lunch-time Python.\")\n",
    "print(type(doc))\n",
    "[i for i in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "t = doc[0]\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"nsubj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    print(t.text, t.pos_, t.dep_, t.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3 Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![pipeline](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)\n",
    "\n",
    "[source: spaCy 101]\n",
    "\n",
    "The capabilities of the processing pipeline dependes on the components, their models and how they were trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Python is a very popular - maybe even the most popular - programming language among scientific software developers. One of the reasons for this success story is the rich standard library and the rich ecosystem of available (scientific) libraries. To fully leverage this ecosystem, developers need to stay up to date and explore new libraries. Lunch Time Python aims at providing a communication platform between Pythonistas to learn about new libraries in an informal setting. Sessions take roughly 30 minutes, one library is presented per session and the code will be made available afterwards. Come by, enjoy your lunch with us and step up your Python game!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print(i, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    for j, token in enumerate(sent):\n",
    "        print(i, j, token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rule-based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "python_pattern = [{\"TEXT\": \"Python\", \"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PYTHON_PATTERN\", [python_pattern])\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word vectors and semantic similarity\n",
    "spaCy can compare two objects and predict similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text1 = \"I like Python.\"\n",
    "text2 = \"I like snakes.\"\n",
    "\n",
    "\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "token1 = doc1[2]\n",
    "token2 = doc2[2]\n",
    "print(token1.text, token2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The similarity score is generated from word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(token1.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarity can be used to predict similar texts to users, or to flag duplicate content. \n",
    "\n",
    "But: Similarity always depends on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text3 = \"I hate snakes.\"\n",
    "doc3 = nlp(text3)\n",
    "print(doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These come out similar as both statements express a sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Internal workings\n",
    "spaCy stores all strings as hash values and creates a lookup table. This way, a word that occurs several times only needs to be stored once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp.vocab.strings.add(\"python\")\n",
    "python_hash = nlp.vocab.strings[\"python\"]\n",
    "python_string = nlp.vocab.strings[python_hash]\n",
    "print(python_hash, python_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- lexemes are entries in the vicabulary and contain context-independent information (the text, hjash, lexical attributes).\n",
    "![data structure](https://course.spacy.io/vocab_stringstore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy demos\n",
    "- You can explore spaCy using [online tools](https://explosion.ai/software)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, the [rule-based matcher explorer](https://demos.explosion.ai/matcher) -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- or the [spaCy online course](https://course.spacy.io/en/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example use cases\n",
    "- [Detection of programming language in stackoverflow posts](https://github.com/koaning/spacy-youtube-material)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebbca54691d61843c0a04253fcd790a2bc545e11985b7cc4dd8a14aab0b5083b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
